{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0a39a94",
   "metadata": {},
   "source": [
    "1. Case Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1da8537a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hey there! We have a new offer for you. Get 50% discount on all products. Limited time offer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1071669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey there! We have a new offer for you. Get 50% discount on all products. Limited time offer.\n"
     ]
    }
   ],
   "source": [
    "text = \"Hey there! We have a new offer for you. Get 50% discount on all products. Limited time offer.\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b7e4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.lower()\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0cee77",
   "metadata": {},
   "source": [
    "2. Punctuation Removal\n",
    "Punctuation often doesn’t contribute to the meaning of a sentence in tasks like sentiment analysis or spam detection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201e6501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c8ecfb",
   "metadata": {},
   "source": [
    "3. Tokenization\n",
    "Tokenization breaks a stream of text into smaller units called tokens. These can be words, sentences, or subwords.\n",
    "\n",
    "Word Tokenization: Separates a text into a list of words.\n",
    "Sentence Tokenization: Separates a text into a list of sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97702cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e55499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Sentence Tokenization\n",
    "sentence_tokens = sent_tokenize(text)\n",
    "print(sentence_tokens)\n",
    "\n",
    "# Word Tokenization\n",
    "words_tokens = word_tokenize(text)\n",
    "print(words_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f169bc",
   "metadata": {},
   "source": [
    "4. Stop Word Removal\n",
    "Stop words are common words like “the,” “is,” “a,” etc., that often don’t add much value to the meaning of the text. Removing them can reduce the feature space and speed up processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de79bddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords if not already present\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in words_tokens if word not in stop_words]\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6cbb3572",
   "metadata": {},
   "source": [
    "5. Stemming & Lemmatization\n",
    "These techniques reduce words to their base or root form, helping to normalize different forms of the same word.\n",
    "Ex:\n",
    "“running” → “run”\n",
    "“goes” → “go”\n",
    "“studies” → “studi” (not a valid word)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0e9f4b55",
   "metadata": {},
   "source": [
    "Lemmatization\n",
    "Lemmatization uses a dictionary and morphological analysis to convert a word to its lemma, or base dictionary form.\n",
    "\n",
    "Ex:\n",
    "“running” → “run” (verb)\n",
    "“is,” “was,” “am” → “be”\n",
    "“geese” → “goose” (handles irregular plurals)\n",
    "“better” → “good” (handles irregular forms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b8bb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Download wordnet if not already present\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "print(stemmed_words)\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3389804",
   "metadata": {},
   "source": [
    "6. Part-of-Speech (POS) Tagging\n",
    "Part-of-Speech (POS) tagging is process of labeling each word in a sentence with its corresponding part of speech, such as a noun, verb, adjective, or adverb.\n",
    "\n",
    "For example:\n",
    "\n",
    "Nouns (NN): Refer to a person, place, thing, or idea (e.g., “dog,” “New York,” “love”).\n",
    "Verbs (VB): Describe an action or state of being (e.g., “run,” “is,” “think”).\n",
    "Adjectives (JJ): Modify or describe nouns (e.g., “happy,” “blue,” “tall”).\n",
    "Adverbs (RB): Modify verbs, adjectives, or other adverbs (e.g., “quickly,” “very,” “well”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29e8586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have the original text processed and word tokenized\n",
    "import nltk\n",
    "text = \"Hey there! We have a new offer for you.\"\n",
    "words = ['hey', 'there', '!', 'we', 'have', 'a', 'new', 'offer', 'for', 'you', '.']\n",
    "\n",
    "# POS Tagging\n",
    "tagged_words = nltk.pos_tag(words)\n",
    "print(\"POS Tagged Words:\", tagged_words)\n",
    "# Example output for a sentence: [('We', 'PRP'), ('have', 'VBP'), ('a', 'DT'), ('new', 'JJ'), ('offer', 'NN'), ('for', 'IN'), ('you', 'PRP'), ('.', '.')]\n",
    "\n",
    "# Define a custom grammar to find noun phrases (NP)\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "parser = nltk.RegexpParser(grammar)\n",
    "result = parser.parse(tagged_words)\n",
    "print(\"Parsed Result:\")\n",
    "print(result)\n",
    "# The parser identifies and groups the words based on the grammar.\n",
    "# For example, it might identify '(NP a/DT new/JJ offer/NN)' as a noun phrase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d21209",
   "metadata": {},
   "source": [
    "spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d06149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Process the text with the nlp object\n",
    "doc = nlp(text)\n",
    "\n",
    "# Iterate through the tokens and print the word and its POS tag\n",
    "for token in doc:\n",
    "    print(f\"{token.text:<10} {token.pos_:<10} {token.tag_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f3ca99",
   "metadata": {},
   "source": [
    "7. Chunking\n",
    "also known as shallow parsing, is a NLP technique that groups words into meaningful phrases, such as noun phrases, verb phrases, or prepositional phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7925c673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print all noun chunks in the document\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c20b24d",
   "metadata": {},
   "source": [
    "8. Named Entity Recognition (NER)\n",
    "NER is a natural language processing (NLP) task that finds and classifies named entities in a text into predefined categories like people, organizations, locations, dates, and more."
   ]
  },
  {
   "cell_type": "raw",
   "id": "78be6613",
   "metadata": {},
   "source": [
    "For example, in the sentence, “Tim Cook, the CEO of Apple, announced the new iPhone in Cupertino on September 9, 2025” an NER system would identify:\n",
    "Tim Cook: Person\n",
    "Apple: Organization\n",
    "Cupertino: Location\n",
    "September 12, 2025: Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e5b8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text\n",
    "text = \"Apple Inc. is a technology company headquartered in Cupertino, California. It was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne.\"\n",
    "\n",
    "# Process the text with the NLP pipeline\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print the identified entities and their labels\n",
    "for ent in doc.ents:\n",
    "    print(f\"Entity: {ent.text}, Label: {ent.label_}, Explanation: {spacy.explain(ent.label_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a561f113",
   "metadata": {},
   "source": [
    "9. Relationship extraction\n",
    "Relationship extraction is an NLP task that identifies and classifies semantic relationships between entities in a text.\n",
    "For example, it can identify that a specific person “works for” a specific company or that a company is “headquartered in” a specific location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcfd81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relationship extraction using spaCy's dependency parsing\n",
    "import spacy\n",
    "\n",
    "# Load the English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text for relationship extraction\n",
    "text = \"Steve Jobs was the CEO of Apple Inc. Apple is headquartered in Cupertino, California.\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "print(\"Relationship Extraction using Dependency Parsing:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Extract relationships based on dependency patterns\n",
    "for token in doc:\n",
    "    # Look for subject-verb-object patterns\n",
    "    if token.dep_ == \"nsubj\":  # nominal subject\n",
    "        subject = token.text\n",
    "        verb = token.head.text\n",
    "        \n",
    "        # Look for objects related to this verb\n",
    "        for child in token.head.children:\n",
    "            if child.dep_ in [\"dobj\", \"attr\", \"pobj\"]:  # direct object, attribute, prepositional object\n",
    "                obj = child.text\n",
    "                print(f\"Relationship: {subject} -> {verb} -> {obj}\")\n",
    "    \n",
    "    # Look for compound relationships\n",
    "    if token.dep_ == \"compound\":\n",
    "        compound = token.text + \" \" + token.head.text\n",
    "        print(f\"Compound entity: {compound}\")\n",
    "\n",
    "print(\"\\nNamed Entities and their relationships:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Print named entities\n",
    "for ent in doc.ents:\n",
    "    print(f\"Entity: {ent.text} ({ent.label_})\")\n",
    "\n",
    "print(\"\\nDependency tree visualization:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Show dependency relationships\n",
    "for token in doc:\n",
    "    print(f\"{token.text:12} <- {token.dep_:10} <- {token.head.text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
