{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b316b1af",
   "metadata": {},
   "source": [
    "\n",
    "Text representation or Feature extraction is the process of converting raw text into a numerical format that a computer can understand and process."
   ]
  },
  {
   "cell_type": "raw",
   "id": "2fa2e9a5",
   "metadata": {},
   "source": [
    "Ex: Corpus: ‚ÄúThe cat sat on the mat.‚Äù and ‚ÄúThe dog sat on the log.‚Äù\n",
    "Vocabulary: {‚Äúthe‚Äù, ‚Äúcat‚Äù, ‚Äúsat‚Äù, ‚Äúon‚Äù, ‚Äúmat‚Äù, ‚Äúdog‚Äù, ‚Äúlog‚Äù}\n",
    "Vector for ‚ÄúThe cat sat on the mat.‚Äù: [2, 1, 1, 1, 1, 0, 0] (counts of each word from the vocabulary)\n",
    "This vector tells us which words are present and how often, but it loses the order of the words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa99322f",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "1.One-hot encoding\n",
    "It is a simple text representation technique that converts categorical data, like words, into a numerical format. It creates a binary vector for each word in a vocabulary, with a length equal to the size of the vocabulary."
   ]
  },
  {
   "cell_type": "raw",
   "id": "6871467f",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Create a Vocabulary: Collect all unique words from your documents and assign each a unique index. For example:\n",
    "Vocabulary = {'the': 0, 'cat': 1, 'sat': 2, 'on': 3, 'mat': 4}\n",
    "\n",
    "Generate Binary Vectors: For each word, create a vector of zeros with a length equal to the vocabulary size.\n",
    "Place a 1 at the index corresponding to the word.\n",
    "The word ‚Äòcat‚Äô is at index 1. Its one-hot vector is: [0, 1, 0, 0, 0]\n",
    "The word ‚Äòsat‚Äô is at index 2. Its one-hot vector is: [0, 0, 1, 0, 0]\n",
    "The word ‚Äòmat‚Äô is at index 4. Its one-hot vector is: [0, 0, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cad644f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['cat' 'dog' 'log' 'mat' 'on' 'sat' 'the']\n",
      "One-Hot Encoded Vectors:\n",
      "[[1 0 0 1 1 1 1]\n",
      " [0 1 1 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\"The cat sat on the mat\", \"The dog sat on the log\"]\n",
    "\n",
    "# Create a CountVectorizer instance\n",
    "# The 'binary=True' argument makes it a one-hot-like encoding (presence/absence)\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "\n",
    "# Fit the vectorizer to the corpus and transform the text\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Get the feature names (the vocabulary)\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the vocabulary\n",
    "print(\"Vocabulary:\", vocabulary)\n",
    "\n",
    "# Print the one-hot encoded vectors (as a sparse matrix)\n",
    "print(\"One-Hot Encoded Vectors:\")\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbbc4d7",
   "metadata": {},
   "source": [
    "2. Bag of Words [BoW]\n",
    "This model is a simple text representation technique that represents a document as an unordered collection of words, or a ‚Äúbag.‚Äù It completely ignores grammar and word order but keeps track of word frequencies."
   ]
  },
  {
   "cell_type": "raw",
   "id": "fbf08896",
   "metadata": {},
   "source": [
    "Example with Spam Email üìß Let‚Äôs say we have 2 emails:\n",
    "\n",
    "Email 1 (Spam): ‚ÄúFree money, claim your prize now!‚Äù\n",
    "Email 2 (Not Spam): ‚ÄúPlease confirm your meeting attendance.‚Äù\n",
    "\n",
    "Vector for Email 1 (Spam):\n",
    "'free': 1\n",
    "'money': 1\n",
    "'claim': 1\n",
    "'your': 1\n",
    "'prize': 1\n",
    "'now': 1\n",
    "All other words: 0\n",
    "BoW Vector: [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
    "\n",
    "Vector for Email 2 (Not Spam):\n",
    "'please': 1\n",
    "'confirm': 1\n",
    "'your': 1\n",
    "'meeting': 1\n",
    "'attendance': 1\n",
    "All other words: 0\n",
    "BoW Vector: [0, 0, 0, 1, 0, 0, 1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "af7e891a",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Bag of n-grams [Bag of n-words]\n",
    "Bag of n-grams is an extension of the BoW model.\n",
    " n-gram is a contiguous sequence of n items from a text.\n",
    " Instead of counting single words (1-grams), it counts sequences of 2 (bigrams), 3 (trigrams), or more words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f084d054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1. Basic Bag of Words (Unigrams) ===\n",
      "Vocabulary: ['attendance' 'claim' 'confirm' 'free' 'meeting' 'money' 'now' 'please'\n",
      " 'prize' 'your']\n",
      "BoW Vectors (Unigrams):\n",
      "[[0 1 0 1 0 1 1 0 1 1]\n",
      " [1 0 1 0 1 0 0 1 0 1]]\n",
      "\n",
      "==================================================\n",
      "=== 2. Bag of Bigrams (2-grams) ===\n",
      "Bigram Vocabulary: ['claim your' 'confirm your' 'free money' 'meeting attendance'\n",
      " 'money claim' 'please confirm' 'prize now' 'your meeting' 'your prize']\n",
      "BoW Vectors (Bigrams):\n",
      "[[1 0 1 0 1 0 1 0 1]\n",
      " [0 1 0 1 0 1 0 1 0]]\n",
      "\n",
      "==================================================\n",
      "=== 3. Bag of Trigrams (3-grams) ===\n",
      "Trigram Vocabulary: ['claim your prize' 'confirm your meeting' 'free money claim'\n",
      " 'money claim your' 'please confirm your' 'your meeting attendance'\n",
      " 'your prize now']\n",
      "BoW Vectors (Trigrams):\n",
      "[[1 0 1 1 0 0 1]\n",
      " [0 1 0 0 1 1 0]]\n",
      "\n",
      "==================================================\n",
      "=== 4. Combined Unigrams + Bigrams ===\n",
      "Combined Vocabulary (Unigrams + Bigrams):\n",
      "['attendance' 'claim' 'claim your' 'confirm' 'confirm your' 'free'\n",
      " 'free money' 'meeting' 'meeting attendance' 'money' 'money claim' 'now'\n",
      " 'please' 'please confirm' 'prize' 'prize now' 'your' 'your meeting'\n",
      " 'your prize']\n",
      "BoW Vectors (Unigrams + Bigrams):\n",
      "[[0 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 1 0 1]\n",
      " [1 0 0 1 1 0 0 1 1 0 0 0 1 1 0 0 1 1 0]]\n",
      "\n",
      "==================================================\n",
      "=== 5. Analysis of N-grams ===\n",
      "Email 1: 'Free money, claim your prize now!'\n",
      "Email 2: 'Please confirm your meeting attendance.'\n",
      "\n",
      "Key Insights:\n",
      "‚Ä¢ Unigrams: Capture individual words\n",
      "‚Ä¢ Bigrams: Capture word pairs like 'free money', 'your prize'\n",
      "‚Ä¢ Trigrams: Capture longer phrases like 'claim your prize'\n",
      "‚Ä¢ Combined: Provides both word-level and phrase-level features\n",
      "‚Ä¢ Higher n-grams capture more context but increase feature dimensionality\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Example emails\n",
    "corpus = [\n",
    "    \"Free money, claim your prize now!\",\n",
    "    \"Please confirm your meeting attendance.\"\n",
    "]\n",
    "\n",
    "print(\"=== 1. Basic Bag of Words (Unigrams) ===\")\n",
    "# Create a CountVectorizer instance for unigrams (single words)\n",
    "vectorizer_unigram = CountVectorizer(binary=False, lowercase=True)\n",
    "\n",
    "# Learn the vocabulary from the corpus and transform the text\n",
    "X_unigram = vectorizer_unigram.fit_transform(corpus)\n",
    "\n",
    "# Print the vocabulary (feature names)\n",
    "print(\"Vocabulary:\", vectorizer_unigram.get_feature_names_out())\n",
    "\n",
    "# Print the BoW matrix (as a dense array for readability)\n",
    "print(\"BoW Vectors (Unigrams):\")\n",
    "print(X_unigram.toarray())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"=== 2. Bag of Bigrams (2-grams) ===\")\n",
    "# Create vectorizer for bigrams only\n",
    "vectorizer_bigram = CountVectorizer(ngram_range=(2, 2), lowercase=True)\n",
    "X_bigram = vectorizer_bigram.fit_transform(corpus)\n",
    "\n",
    "print(\"Bigram Vocabulary:\", vectorizer_bigram.get_feature_names_out())\n",
    "print(\"BoW Vectors (Bigrams):\")\n",
    "print(X_bigram.toarray())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"=== 3. Bag of Trigrams (3-grams) ===\")\n",
    "# Create vectorizer for trigrams only\n",
    "vectorizer_trigram = CountVectorizer(ngram_range=(3, 3), lowercase=True)\n",
    "X_trigram = vectorizer_trigram.fit_transform(corpus)\n",
    "\n",
    "print(\"Trigram Vocabulary:\", vectorizer_trigram.get_feature_names_out())\n",
    "print(\"BoW Vectors (Trigrams):\")\n",
    "print(X_trigram.toarray())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"=== 4. Combined Unigrams + Bigrams ===\")\n",
    "# Create vectorizer for both unigrams and bigrams\n",
    "vectorizer_combined = CountVectorizer(ngram_range=(1, 2), lowercase=True)\n",
    "X_combined = vectorizer_combined.fit_transform(corpus)\n",
    "\n",
    "print(\"Combined Vocabulary (Unigrams + Bigrams):\")\n",
    "print(vectorizer_combined.get_feature_names_out())\n",
    "print(\"BoW Vectors (Unigrams + Bigrams):\")\n",
    "print(X_combined.toarray())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"=== 5. Analysis of N-grams ===\")\n",
    "print(\"Email 1: 'Free money, claim your prize now!'\")\n",
    "print(\"Email 2: 'Please confirm your meeting attendance.'\")\n",
    "print()\n",
    "print(\"Key Insights:\")\n",
    "print(\"‚Ä¢ Unigrams: Capture individual words\")\n",
    "print(\"‚Ä¢ Bigrams: Capture word pairs like 'free money', 'your prize'\")\n",
    "print(\"‚Ä¢ Trigrams: Capture longer phrases like 'claim your prize'\")\n",
    "print(\"‚Ä¢ Combined: Provides both word-level and phrase-level features\")\n",
    "print(\"‚Ä¢ Higher n-grams capture more context but increase feature dimensionality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a863456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== N-grams for Sentiment Analysis ===\n",
      "Reviews:\n",
      "1. This movie is really good and entertaining\n",
      "2. The movie is not good at all\n",
      "3. Really bad movie, not entertaining\n",
      "4. This is a really bad experience\n",
      "5. Good movie, really entertaining and good\n",
      "\n",
      "============================================================\n",
      "\n",
      "=== Unigrams (1, 1) ===\n",
      "Number of features: 13\n",
      "Features: ['all', 'and', 'at', 'bad', 'entertaining', 'experience', 'good', 'is', 'movie', 'not', 'really', 'the', 'this']\n",
      "Matrix shape: (5, 13)\n",
      "Feature Matrix:\n",
      "          all  and  at  bad  entertaining  experience  good  is  movie  not  \\\n",
      "Review 1    0    1   0    0             1           0     1   1      1    0   \n",
      "Review 2    1    0   1    0             0           0     1   1      1    1   \n",
      "Review 3    0    0   0    1             1           0     0   0      1    1   \n",
      "Review 4    0    0   0    1             0           1     0   1      0    0   \n",
      "Review 5    0    1   0    0             1           0     2   0      1    0   \n",
      "\n",
      "          really  the  this  \n",
      "Review 1       1    0     1  \n",
      "Review 2       0    1     0  \n",
      "Review 3       1    0     0  \n",
      "Review 4       1    0     1  \n",
      "Review 5       1    0     0  \n",
      "\n",
      "=== Bigrams (2, 2) ===\n",
      "Number of features: 22\n",
      "Features: ['and entertaining', 'and good', 'at all', 'bad experience', 'bad movie', 'entertaining and', 'good and', 'good at', 'good movie', 'is not', 'is really', 'movie is', 'movie not', 'movie really', 'not entertaining', 'not good', 'really bad', 'really entertaining', 'really good', 'the movie', 'this is', 'this movie']\n",
      "Matrix shape: (5, 22)\n",
      "Matrix too large to display - showing first 5 features only\n",
      "          and entertaining  and good  at all  bad experience  bad movie\n",
      "Review 1                 1         0       0               0          0\n",
      "Review 2                 0         0       1               0          0\n",
      "Review 3                 0         0       0               0          1\n",
      "Review 4                 0         0       0               1          0\n",
      "Review 5                 0         1       0               0          0\n",
      "\n",
      "=== Trigrams (3, 3) ===\n",
      "Number of features: 20\n",
      "Features: ['bad movie not', 'entertaining and good', 'good and entertaining', 'good at all', 'good movie really', 'is not good', 'is really bad', 'is really good', 'movie is not', 'movie is really', 'movie not entertaining', 'movie really entertaining', 'not good at', 'really bad experience', 'really bad movie', 'really entertaining and', 'really good and', 'the movie is', 'this is really', 'this movie is']\n",
      "Matrix shape: (5, 20)\n",
      "Matrix too large to display - showing first 5 features only\n",
      "          bad movie not  entertaining and good  good and entertaining  \\\n",
      "Review 1              0                      0                      1   \n",
      "Review 2              0                      0                      0   \n",
      "Review 3              1                      0                      0   \n",
      "Review 4              0                      0                      0   \n",
      "Review 5              0                      1                      0   \n",
      "\n",
      "          good at all  good movie really  \n",
      "Review 1            0                  0  \n",
      "Review 2            1                  0  \n",
      "Review 3            0                  0  \n",
      "Review 4            0                  0  \n",
      "Review 5            0                  1  \n",
      "\n",
      "=== Uni+Bigrams (1, 2) ===\n",
      "Number of features: 35\n",
      "Features: ['all', 'and', 'and entertaining', 'and good', 'at', 'at all', 'bad', 'bad experience', 'bad movie', 'entertaining', 'entertaining and', 'experience', 'good', 'good and', 'good at', 'good movie', 'is', 'is not', 'is really', 'movie', 'movie is', 'movie not', 'movie really', 'not', 'not entertaining', 'not good', 'really', 'really bad', 'really entertaining', 'really good', 'the', 'the movie', 'this', 'this is', 'this movie']\n",
      "Matrix shape: (5, 35)\n",
      "Matrix too large to display - showing first 5 features only\n",
      "          all  and  and entertaining  and good  at\n",
      "Review 1    0    1                 1         0   0\n",
      "Review 2    1    0                 0         0   1\n",
      "Review 3    0    0                 0         0   0\n",
      "Review 4    0    0                 0         0   0\n",
      "Review 5    0    1                 0         1   0\n",
      "\n",
      "=== Uni+Bi+Trigrams (1, 3) ===\n",
      "Number of features: 55\n",
      "Features: ['all', 'and', 'and entertaining', 'and good', 'at', 'at all', 'bad', 'bad experience', 'bad movie', 'bad movie not', 'entertaining', 'entertaining and', 'entertaining and good', 'experience', 'good', 'good and', 'good and entertaining', 'good at', 'good at all', 'good movie', 'good movie really', 'is', 'is not', 'is not good', 'is really', 'is really bad', 'is really good', 'movie', 'movie is', 'movie is not', 'movie is really', 'movie not', 'movie not entertaining', 'movie really', 'movie really entertaining', 'not', 'not entertaining', 'not good', 'not good at', 'really', 'really bad', 'really bad experience', 'really bad movie', 'really entertaining', 'really entertaining and', 'really good', 'really good and', 'the', 'the movie', 'the movie is', 'this', 'this is', 'this is really', 'this movie', 'this movie is']\n",
      "Matrix shape: (5, 55)\n",
      "Matrix too large to display - showing first 5 features only\n",
      "          all  and  and entertaining  and good  at\n",
      "Review 1    0    1                 1         0   0\n",
      "Review 2    1    0                 0         0   1\n",
      "Review 3    0    0                 0         0   0\n",
      "Review 4    0    0                 0         0   0\n",
      "Review 5    0    1                 0         1   0\n",
      "\n",
      "============================================================\n",
      "=== Key Observations ===\n",
      "‚Ä¢ Unigrams: 'good', 'bad', 'really' - basic sentiment words\n",
      "‚Ä¢ Bigrams: 'really good', 'really bad', 'not good' - stronger sentiment context\n",
      "‚Ä¢ Trigrams: 'is really good', 'not good at' - even more context\n",
      "‚Ä¢ Trade-off: More n-grams = more features but better context capture\n"
     ]
    }
   ],
   "source": [
    "# Practical N-grams Example: Sentiment Analysis Context\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# More realistic text examples\n",
    "reviews = [\n",
    "    \"This movie is really good and entertaining\",\n",
    "    \"The movie is not good at all\",\n",
    "    \"Really bad movie, not entertaining\",\n",
    "    \"This is a really bad experience\",\n",
    "    \"Good movie, really entertaining and good\"\n",
    "]\n",
    "\n",
    "print(\"=== N-grams for Sentiment Analysis ===\")\n",
    "print(\"Reviews:\")\n",
    "for i, review in enumerate(reviews, 1):\n",
    "    print(f\"{i}. {review}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Demonstrate different n-gram ranges\n",
    "n_gram_ranges = [(1, 1), (2, 2), (3, 3), (1, 2), (1, 3)]\n",
    "range_names = [\"Unigrams\", \"Bigrams\", \"Trigrams\", \"Uni+Bigrams\", \"Uni+Bi+Trigrams\"]\n",
    "\n",
    "for ngram_range, name in zip(n_gram_ranges, range_names):\n",
    "    print(f\"\\n=== {name} {ngram_range} ===\")\n",
    "    \n",
    "    vectorizer = CountVectorizer(ngram_range=ngram_range, lowercase=True)\n",
    "    X = vectorizer.fit_transform(reviews)\n",
    "    \n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    print(f\"Number of features: {len(feature_names)}\")\n",
    "    print(f\"Features: {list(feature_names)}\")\n",
    "    \n",
    "    # Show matrix shape and some sample values\n",
    "    print(f\"Matrix shape: {X.shape}\")\n",
    "    \n",
    "    # Convert to dense for better readability (only for small examples)\n",
    "    if len(feature_names) <= 15:  # Only show dense matrix if not too large\n",
    "        print(\"Feature Matrix:\")\n",
    "        df = pd.DataFrame(X.toarray(), \n",
    "                         columns=feature_names,\n",
    "                         index=[f\"Review {i+1}\" for i in range(len(reviews))])\n",
    "        print(df)\n",
    "    else:\n",
    "        print(\"Matrix too large to display - showing first 5 features only\")\n",
    "        df = pd.DataFrame(X.toarray()[:, :5], \n",
    "                         columns=feature_names[:5],\n",
    "                         index=[f\"Review {i+1}\" for i in range(len(reviews))])\n",
    "        print(df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== Key Observations ===\")\n",
    "print(\"‚Ä¢ Unigrams: 'good', 'bad', 'really' - basic sentiment words\")\n",
    "print(\"‚Ä¢ Bigrams: 'really good', 'really bad', 'not good' - stronger sentiment context\") \n",
    "print(\"‚Ä¢ Trigrams: 'is really good', 'not good at' - even more context\")\n",
    "print(\"‚Ä¢ Trade-off: More n-grams = more features but better context capture\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c254a6c",
   "metadata": {},
   "source": [
    "3. TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "TF-IDF is a numerical statistic used to reflect how important a word is to a document in a corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776ea0bb",
   "metadata": {},
   "source": [
    "Term Frequency measures how often a word appears in a specific document. The more frequent the word, the higher its TF score\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dc48a1bb",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Drawbacks of TF-IDF\n",
    "\n",
    "TF-IDF only considers word frequency and not the meaning or context. \n",
    "It can‚Äôt tell the difference between ‚Äúcar‚Äù and ‚Äúautomobile,‚Äù which are semantically similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f98ceaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['also' 'applications' 'developers' 'easy' 'enterprise' 'for' 'is' 'java'\n",
      " 'language' 'learn' 'popular' 'programming' 'python' 'to' 'use']\n",
      "\n",
      "TF-IDF Vectors:\n",
      "[[0.         0.         0.         0.17008209 0.         0.\n",
      "  0.36304442 0.         0.12101481 0.17008209 0.17008209 0.12101481\n",
      "  0.85041044 0.17008209 0.        ]\n",
      " [0.24604336 0.24604336 0.24604336 0.         0.24604336 0.24604336\n",
      "  0.17506188 0.73813008 0.17506188 0.         0.         0.17506188\n",
      "  0.         0.         0.24604336]]\n",
      "\n",
      "============================================================\n",
      "=== DETAILED TF-IDF ANALYSIS ===\n",
      "\n",
      "TF-IDF Score Matrix:\n",
      "                              also  applications  developers    easy  \\\n",
      "Document 1 (Python-focused)  0.000         0.000       0.000  0.1701   \n",
      "Document 2 (Java-focused)    0.246         0.246       0.246  0.0000   \n",
      "\n",
      "                             enterprise    for      is    java  language  \\\n",
      "Document 1 (Python-focused)       0.000  0.000  0.3630  0.0000    0.1210   \n",
      "Document 2 (Java-focused)         0.246  0.246  0.1751  0.7381    0.1751   \n",
      "\n",
      "                              learn  popular  programming  python      to  \\\n",
      "Document 1 (Python-focused)  0.1701   0.1701       0.1210  0.8504  0.1701   \n",
      "Document 2 (Java-focused)    0.0000   0.0000       0.1751  0.0000  0.0000   \n",
      "\n",
      "                               use  \n",
      "Document 1 (Python-focused)  0.000  \n",
      "Document 2 (Java-focused)    0.246  \n",
      "\n",
      "=== TOP WORDS BY TF-IDF SCORE ===\n",
      "\n",
      "Document 1 (Python-focused):\n",
      "  python: 0.8504\n",
      "  is: 0.3630\n",
      "  easy: 0.1701\n",
      "  learn: 0.1701\n",
      "  popular: 0.1701\n",
      "\n",
      "Document 2 (Java-focused):\n",
      "  java: 0.7381\n",
      "  also: 0.2460\n",
      "  applications: 0.2460\n",
      "  developers: 0.2460\n",
      "  enterprise: 0.2460\n",
      "\n",
      "============================================================\n",
      "=== RAW TERM FREQUENCIES (TF) ===\n",
      "Term Frequency Matrix (raw counts):\n",
      "            also  applications  developers  easy  enterprise  for  is  java  \\\n",
      "Document 1     0             0           0     1           0    0   3     0   \n",
      "Document 2     1             1           1     0           1    1   1     3   \n",
      "\n",
      "            language  learn  popular  programming  python  to  use  \n",
      "Document 1         1      1        1            1       5   1    0  \n",
      "Document 2         1      0        0            1       0   0    1  \n",
      "\n",
      "=== KEY OBSERVATIONS ===\n",
      "‚Ä¢ 'Python' appears 5 times in Document 1 ‚Üí High TF score\n",
      "‚Ä¢ 'Java' appears 3 times in Document 2 ‚Üí High TF score\n",
      "‚Ä¢ 'programming' and 'language' appear in both ‚Üí Lower IDF, moderate TF-IDF\n",
      "‚Ä¢ Words like 'easy', 'learn', 'enterprise' are unique ‚Üí Higher IDF, higher TF-IDF\n",
      "‚Ä¢ TF-IDF = TF √ó IDF (balances frequency with rarity)\n",
      "\n",
      "IDF Scores (Inverse Document Frequency):\n",
      "also: 1.4055\n",
      "applications: 1.4055\n",
      "developers: 1.4055\n",
      "easy: 1.4055\n",
      "enterprise: 1.4055\n",
      "for: 1.4055\n",
      "is: 1.0000\n",
      "java: 1.4055\n",
      "language: 1.0000\n",
      "learn: 1.4055\n",
      "popular: 1.4055\n",
      "programming: 1.0000\n",
      "python: 1.4055\n",
      "to: 1.4055\n",
      "use: 1.4055\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Example corpus designed to show different TF scores\n",
    "corpus = [\n",
    "    \"Python is a programming language. Python is easy to learn. Python Python Python is popular.\",  # Document 1: High TF for \"Python\"\n",
    "    \"Java is also a programming language. Java developers use Java for enterprise applications.\"     # Document 2: High TF for \"Java\"\n",
    "]\n",
    "\n",
    "# Create a TfidfVectorizer instance\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the corpus\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Get the feature names (the vocabulary)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the vocabulary\n",
    "print(\"Vocabulary:\", feature_names)\n",
    "\n",
    "# Print the TF-IDF vectors (as a dense array for readability)\n",
    "print(\"\\nTF-IDF Vectors:\")\n",
    "tfidf_matrix = X.toarray()\n",
    "print(tfidf_matrix)\n",
    "\n",
    "# Create a detailed analysis showing TF scores\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== DETAILED TF-IDF ANALYSIS ===\")\n",
    "\n",
    "# Show TF-IDF scores for each document\n",
    "import pandas as pd\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "df = pd.DataFrame(tfidf_matrix, \n",
    "                 columns=feature_names,\n",
    "                 index=['Document 1 (Python-focused)', 'Document 2 (Java-focused)'])\n",
    "\n",
    "print(\"\\nTF-IDF Score Matrix:\")\n",
    "print(df.round(4))\n",
    "\n",
    "# Analyze highest scoring words in each document\n",
    "print(\"\\n=== TOP WORDS BY TF-IDF SCORE ===\")\n",
    "for i, doc_name in enumerate(['Document 1 (Python-focused)', 'Document 2 (Java-focused)']):\n",
    "    print(f\"\\n{doc_name}:\")\n",
    "    word_scores = [(feature_names[j], tfidf_matrix[i][j]) for j in range(len(feature_names))]\n",
    "    word_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for word, score in word_scores[:5]:  # Top 5 words\n",
    "        if score > 0:\n",
    "            print(f\"  {word}: {score:.4f}\")\n",
    "\n",
    "# Show raw term frequencies for comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== RAW TERM FREQUENCIES (TF) ===\")\n",
    "\n",
    "# Calculate raw TF using CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "tf_matrix = count_vectorizer.fit_transform(corpus).toarray()\n",
    "\n",
    "tf_df = pd.DataFrame(tf_matrix, \n",
    "                    columns=count_vectorizer.get_feature_names_out(),\n",
    "                    index=['Document 1', 'Document 2'])\n",
    "\n",
    "print(\"Term Frequency Matrix (raw counts):\")\n",
    "print(tf_df)\n",
    "\n",
    "print(\"\\n=== KEY OBSERVATIONS ===\")\n",
    "print(\"‚Ä¢ 'Python' appears 5 times in Document 1 ‚Üí High TF score\")\n",
    "print(\"‚Ä¢ 'Java' appears 3 times in Document 2 ‚Üí High TF score\") \n",
    "print(\"‚Ä¢ 'programming' and 'language' appear in both ‚Üí Lower IDF, moderate TF-IDF\")\n",
    "print(\"‚Ä¢ Words like 'easy', 'learn', 'enterprise' are unique ‚Üí Higher IDF, higher TF-IDF\")\n",
    "print(\"‚Ä¢ TF-IDF = TF √ó IDF (balances frequency with rarity)\")\n",
    "\n",
    "# You can also get the IDF scores\n",
    "print(\"\\nIDF Scores (Inverse Document Frequency):\")\n",
    "for i, name in enumerate(feature_names):\n",
    "    print(f\"{name}: {vectorizer.idf_[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d7953900",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "One-Hot Encoding, Bag-of-Words (BoW), and TF-IDF suffer from a major drawback:\n",
    "they fail to capture the semantic relationships between words.\n",
    "\n",
    "One-Hot Encoding ‚Äî Creates extremely large and sparse vectors. \n",
    "It treats every word as a completely independent entity,\n",
    "so there‚Äôs no way to tell that ‚Äúking‚Äù and ‚Äúqueen‚Äù are related.\n",
    "\n",
    "Bag-of-Words ‚Äî Ignores word order and context. \n",
    "It counts words but loses the sentence structure.\n",
    "‚ÄúThe dog bit the man‚Äù and ‚ÄúThe man bit the dog‚Äù have the same representation.\n",
    "\n",
    "TF-IDF ‚Äî Improves on BoW by weighting important words,\n",
    "but still doesn‚Äôt understand semantics. \n",
    "‚ÄúCar‚Äù and ‚Äúautomobile‚Äù have completely different representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56538aa2",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "4. Word Embedding\n",
    "Word Embedding is a text representation technique where words or phrases from the\n",
    " vocabulary are mapped to vectors of real numbers."
   ]
  },
  {
   "cell_type": "raw",
   "id": "440660ad",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Types of Word Embedding Techniques\n",
    "\n",
    "1.Word2Vec:\n",
    "Word2Vec is a neural network-based text representation technique that learns to\n",
    " represent words as dense vectors, called word embeddings."
   ]
  },
  {
   "cell_type": "raw",
   "id": "100577d1",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Word2Vec has 2 main architectures: Continuous Bag-of-Words (CBOW) and Skip-gram.\n",
    " Both models use a shallow neural network to learn the word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b39c2b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'cat':\n",
      " [ 0.00257251  0.00084847 -0.00253914  0.00935892  0.00275784  0.00409409\n",
      " -0.00118331  0.00090606  0.00662316 -0.00072743  0.00334267 -0.00067134\n",
      "  0.00524796  0.00363926  0.002584   -0.0053113  -0.00470893  0.00430647\n",
      " -0.0059082  -0.00018227 -0.00063462  0.00349116 -0.00844191  0.00881516\n",
      " -0.00145111 -0.00533294  0.00405283 -0.00193385 -0.0077646  -0.00449672\n",
      " -0.00038841 -0.00894825  0.00057069  0.00244194 -0.00322519  0.00257062\n",
      "  0.00248097  0.00998819  0.00142857  0.0020191   0.00277751 -0.0020782\n",
      " -0.0086982   0.00802494 -0.00197519 -0.0096929  -0.00654969 -0.00394582\n",
      "  0.00395376  0.00504065  0.00608667 -0.00677156  0.00069044 -0.00277415\n",
      " -0.0052109   0.0069812   0.00395213 -0.00310513 -0.00827734 -0.00514148\n",
      " -0.00064909  0.007812    0.00604447 -0.00845231 -0.009565    0.00713558\n",
      " -0.00232571 -0.00369028  0.00574776 -0.00584365  0.00509312 -0.00024008\n",
      " -0.00687449 -0.0003305   0.00635958  0.00929556  0.00221949  0.00505199\n",
      " -0.00497511 -0.00079866 -0.0053181   0.00119131 -0.00179489 -0.00363314\n",
      " -0.00701271  0.00965442  0.00297428 -0.00228278 -0.00418191  0.00771538\n",
      " -0.00648072  0.00312075  0.00078511  0.00832376  0.00683792 -0.00290998\n",
      "  0.00253321 -0.00166683 -0.00945585 -0.00261407]\n",
      "\n",
      "Words most similar to 'cat':\n",
      " [('of', 0.14866504073143005), ('truck', 0.11212983727455139), ('walked', 0.10753479599952698), ('family', 0.09773248434066772), ('the', 0.08435528725385666), ('street', 0.03175520524382591), ('is', 0.03162171319127083), ('mat', 0.014369957149028778), ('on', 0.013860994949936867), ('man', 0.009801937267184258)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample corpus (tokenized sentences)\n",
    "corpus = [\n",
    "    [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n",
    "    [\"the\", \"dog\", \"walked\", \"on\", \"the\", \"street\"],\n",
    "    [\"a\", \"car\", \"drove\", \"by\", \"a\", \"truck\"],\n",
    "    [\"man\", \"is\", \"the\", \"king\", \"of\", \"jungle\"],\n",
    "    [\"woman\", \"is\", \"the\", \"queen\", \"of\", \"a\", \"family\"]\n",
    "]\n",
    "\n",
    "# Train the Word2Vec model\n",
    "# We use the default Skip-gram model (sg=0 for CBOW)\n",
    "model = Word2Vec(sentences=corpus,\n",
    "                 vector_size=100,  # Dimensionality of the word vectors\n",
    "                 window=5,         # Maximum distance between the current and predicted word\n",
    "                 min_count=1,      # Ignores all words with total frequency lower than this\n",
    "                 workers=4)        # Use 4 CPU cores for training\n",
    "\n",
    "# Get the vector for a word\n",
    "print(\"Vector for 'cat':\\n\", model.wv['cat'])\n",
    "\n",
    "# Find the most similar words\n",
    "print(\"\\nWords most similar to 'cat':\\n\", model.wv.most_similar('cat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f155a2b7",
   "metadata": {},
   "source": [
    "### Word2Vec Code Explanation\n",
    "\n",
    "The following code demonstrates how to train and use a Word2Vec model using the Gensim library. Word2Vec creates dense vector representations of words by learning from their context in sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afbe0e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STEP 1: DATA PREPARATION ===\n",
      "Training corpus (each sentence is a list of words):\n",
      "  Sentence 1: ['the', 'cat', 'sat', 'on', 'the', 'mat']\n",
      "  Sentence 2: ['the', 'dog', 'walked', 'on', 'the', 'street']\n",
      "  Sentence 3: ['a', 'car', 'drove', 'by', 'a', 'truck']\n",
      "  Sentence 4: ['man', 'is', 'the', 'king', 'of', 'jungle']\n",
      "  Sentence 5: ['woman', 'is', 'the', 'queen', 'of', 'a', 'family']\n",
      "\n",
      "Total sentences: 5\n",
      "Total unique words: 21\n",
      "\n",
      "============================================================\n",
      "=== STEP 2: MODEL TRAINING ===\n",
      "Model training completed!\n",
      "Vocabulary size: 21\n",
      "Vector dimensions: 100\n",
      "\n",
      "============================================================\n",
      "=== STEP 3: EXPLORING WORD VECTORS ===\n",
      "Vector for 'cat' (first 10 dimensions):\n",
      "Shape: (100,)\n",
      "Values: [ 0.00257251  0.00084847 -0.00253914  0.00935892  0.00275784  0.00409409\n",
      " -0.00118331  0.00090606  0.00662316 -0.00072743]\n",
      "\n",
      "============================================================\n",
      "=== STEP 4: FINDING SIMILAR WORDS ===\n",
      "Words most similar to 'cat':\n",
      "  of: 0.1487\n",
      "  truck: 0.1121\n",
      "  walked: 0.1075\n",
      "\n",
      "============================================================\n",
      "=== STEP 5: UNDERSTANDING THE PARAMETERS ===\n",
      " KEY PARAMETERS EXPLAINED:\n",
      "‚Ä¢ vector_size=100: Each word ‚Üí 100-dimensional dense vector\n",
      "‚Ä¢ window=5: Uses 5 words before + 5 words after for context\n",
      "‚Ä¢ min_count=1: Include words appearing ‚â•1 times (use higher for large corpora)\n",
      "‚Ä¢ workers=4: Parallel processing using 4 CPU cores\n",
      "‚Ä¢ sg=0: CBOW architecture (sg=1 would be Skip-gram)\n",
      "\n",
      " WHAT HAPPENS DURING TRAINING:\n",
      "1. Model sees: ['the', 'cat', 'sat', 'on', 'the']\n",
      "2. For word 'cat', context window includes: ['the', 'sat', 'on', 'the']\n",
      "3. Neural network learns: 'cat' should be similar to words in similar contexts\n",
      "4. Result: Words with similar contexts get similar vectors\n",
      "\n",
      " SEMANTIC RELATIONSHIPS CAPTURED:\n",
      "‚Ä¢ 'cat' and 'dog' should be similar (both animals)\n",
      "‚Ä¢ 'king' and 'queen' should be similar (both royalty)\n",
      "‚Ä¢ 'sat' and 'walked' should be similar (both actions)\n"
     ]
    }
   ],
   "source": [
    "# Let's break down the Word2Vec code step by step\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "print(\"=== STEP 1: DATA PREPARATION ===\")\n",
    "# Sample corpus (tokenized sentences)\n",
    "# Important: Word2Vec requires PRE-TOKENIZED text (list of lists)\n",
    "corpus = [\n",
    "    [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],        # Sentence 1: tokenized into words\n",
    "    [\"the\", \"dog\", \"walked\", \"on\", \"the\", \"street\"],  # Sentence 2: tokenized into words\n",
    "    [\"a\", \"car\", \"drove\", \"by\", \"a\", \"truck\"],        # Sentence 3: tokenized into words\n",
    "    [\"man\", \"is\", \"the\", \"king\", \"of\", \"jungle\"],     # Sentence 4: tokenized into words\n",
    "    [\"woman\", \"is\", \"the\", \"queen\", \"of\", \"a\", \"family\"] # Sentence 5: tokenized into words\n",
    "]\n",
    "\n",
    "print(\"Training corpus (each sentence is a list of words):\")\n",
    "for i, sentence in enumerate(corpus, 1):\n",
    "    print(f\"  Sentence {i}: {sentence}\")\n",
    "\n",
    "print(f\"\\nTotal sentences: {len(corpus)}\")\n",
    "print(f\"Total unique words: {len(set([word for sentence in corpus for word in sentence]))}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== STEP 2: MODEL TRAINING ===\")\n",
    "\n",
    "# Train the Word2Vec model with detailed explanation of each parameter\n",
    "model = Word2Vec(\n",
    "    sentences=corpus,        # Input: list of tokenized sentences\n",
    "    vector_size=100,        # Output: each word becomes a 100-dimensional vector\n",
    "    window=5,               # Context: look at 5 words before and after target word\n",
    "    min_count=1,            # Vocabulary: include words that appear at least 1 time\n",
    "    workers=4,              # Performance: use 4 CPU cores for training\n",
    "    sg=0                    # Architecture: sg=0 for CBOW, sg=1 for Skip-gram\n",
    ")\n",
    "\n",
    "print(\"Model training completed!\")\n",
    "print(f\"Vocabulary size: {len(model.wv.key_to_index)}\")\n",
    "print(f\"Vector dimensions: {model.wv.vector_size}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== STEP 3: EXPLORING WORD VECTORS ===\")\n",
    "\n",
    "# Get the vector for a specific word\n",
    "word = 'cat'\n",
    "vector = model.wv[word]\n",
    "print(f\"Vector for '{word}' (first 10 dimensions):\")\n",
    "print(f\"Shape: {vector.shape}\")\n",
    "print(f\"Values: {vector[:10]}\")  # Show only first 10 dimensions for readability\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== STEP 4: FINDING SIMILAR WORDS ===\")\n",
    "\n",
    "# Find words most similar to 'cat'\n",
    "try:\n",
    "    similar_words = model.wv.most_similar('cat', topn=3)\n",
    "    print(f\"Words most similar to 'cat':\")\n",
    "    for word, similarity in similar_words:\n",
    "        print(f\"  {word}: {similarity:.4f}\")\n",
    "except:\n",
    "    print(\"Not enough data to compute similarities (need larger corpus)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== STEP 5: UNDERSTANDING THE PARAMETERS ===\")\n",
    "print(\" KEY PARAMETERS EXPLAINED:\")\n",
    "print(\"‚Ä¢ vector_size=100: Each word ‚Üí 100-dimensional dense vector\")\n",
    "print(\"‚Ä¢ window=5: Uses 5 words before + 5 words after for context\")\n",
    "print(\"‚Ä¢ min_count=1: Include words appearing ‚â•1 times (use higher for large corpora)\")\n",
    "print(\"‚Ä¢ workers=4: Parallel processing using 4 CPU cores\")\n",
    "print(\"‚Ä¢ sg=0: CBOW architecture (sg=1 would be Skip-gram)\")\n",
    "\n",
    "print(\"\\n WHAT HAPPENS DURING TRAINING:\")\n",
    "print(\"1. Model sees: ['the', 'cat', 'sat', 'on', 'the']\")\n",
    "print(\"2. For word 'cat', context window includes: ['the', 'sat', 'on', 'the']\")\n",
    "print(\"3. Neural network learns: 'cat' should be similar to words in similar contexts\")\n",
    "print(\"4. Result: Words with similar contexts get similar vectors\")\n",
    "\n",
    "print(\"\\n SEMANTIC RELATIONSHIPS CAPTURED:\")\n",
    "print(\"‚Ä¢ 'cat' and 'dog' should be similar (both animals)\")\n",
    "print(\"‚Ä¢ 'king' and 'queen' should be similar (both royalty)\")\n",
    "print(\"‚Ä¢ 'sat' and 'walked' should be similar (both actions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8f5340",
   "metadata": {},
   "source": [
    "### Key Differences: CBOW vs Skip-gram\n",
    "\n",
    "**CBOW (Continuous Bag of Words) - sg=0:**\n",
    "- **Input**: Context words ‚Üí **Output**: Target word\n",
    "- **Example**: Given [\"the\", \"sat\", \"on\", \"the\"] ‚Üí Predict \"cat\"\n",
    "- **Speed**: Faster training\n",
    "- **Best for**: Frequent words, smaller datasets\n",
    "\n",
    "**Skip-gram - sg=1:**\n",
    "- **Input**: Target word ‚Üí **Output**: Context words  \n",
    "- **Example**: Given \"cat\" ‚Üí Predict [\"the\", \"sat\", \"on\", \"the\"]\n",
    "- **Quality**: Better word representations\n",
    "- **Best for**: Rare words, larger datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd99f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBOW (Continuous Bag of Words) Example Program\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "print(\"=== CBOW ARCHITECTURE DEMONSTRATION ===\")\n",
    "print(\"CBOW: Context Words ‚Üí Target Word\")\n",
    "print()\n",
    "\n",
    "# Larger corpus for better CBOW demonstration\n",
    "sentences = [\n",
    "    [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"],\n",
    "    [\"a\", \"brown\", \"fox\", \"runs\", \"quickly\", \"through\", \"the\", \"forest\"],\n",
    "    [\"the\", \"lazy\", \"dog\", \"sleeps\", \"under\", \"the\", \"warm\", \"sun\"],\n",
    "    [\"quick\", \"animals\", \"jump\", \"over\", \"obstacles\", \"in\", \"the\", \"wild\"],\n",
    "    [\"brown\", \"animals\", \"run\", \"fast\", \"through\", \"green\", \"forests\"],\n",
    "    [\"lazy\", \"pets\", \"sleep\", \"peacefully\", \"in\", \"warm\", \"houses\"],\n",
    "    [\"the\", \"dog\", \"and\", \"fox\", \"are\", \"both\", \"smart\", \"animals\"],\n",
    "    [\"quick\", \"brown\", \"foxes\", \"are\", \"clever\", \"forest\", \"animals\"]\n",
    "]\n",
    "\n",
    "print(\"Training corpus:\")\n",
    "for i, sentence in enumerate(sentences[:3], 1):  # Show first 3 sentences\n",
    "    print(f\"  {i}. {' '.join(sentence)}\")\n",
    "print(f\"  ... and {len(sentences)-3} more sentences\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== TRAINING CBOW MODEL ===\")\n",
    "\n",
    "# Train CBOW model (sg=0)\n",
    "cbow_model = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=50,      # Smaller for demonstration\n",
    "    window=3,            # Context window of 3 words on each side\n",
    "    min_count=1,         # Include all words\n",
    "    workers=1,           # Single thread for consistent results\n",
    "    sg=0,                # CBOW architecture\n",
    "    epochs=100           # More training epochs\n",
    ")\n",
    "\n",
    "print(\"CBOW Model trained successfully!\")\n",
    "print(f\"Vocabulary size: {len(cbow_model.wv.key_to_index)}\")\n",
    "print(f\"Vector dimensions: {cbow_model.wv.vector_size}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== HOW CBOW WORKS: STEP BY STEP ===\")\n",
    "\n",
    "# Demonstrate CBOW concept with a specific example\n",
    "target_word = \"fox\"\n",
    "example_sentence = [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\"]\n",
    "window_size = 3\n",
    "\n",
    "print(f\"Example sentence: {' '.join(example_sentence)}\")\n",
    "print(f\"Target word: '{target_word}'\")\n",
    "print(f\"Window size: {window_size}\")\n",
    "\n",
    "# Find the target word position\n",
    "target_index = example_sentence.index(target_word)\n",
    "print(f\"Target word position: {target_index}\")\n",
    "\n",
    "# Extract context words\n",
    "start_idx = max(0, target_index - window_size)\n",
    "end_idx = min(len(example_sentence), target_index + window_size + 1)\n",
    "context_words = [example_sentence[i] for i in range(start_idx, end_idx) if i != target_index]\n",
    "\n",
    "print(f\"Context words: {context_words}\")\n",
    "print(f\"CBOW Learning: {context_words} ‚Üí '{target_word}'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== WORD SIMILARITIES (CBOW) ===\")\n",
    "\n",
    "# Test word similarities\n",
    "test_words = ['fox', 'dog', 'brown', 'quick']\n",
    "for word in test_words:\n",
    "    if word in cbow_model.wv:\n",
    "        try:\n",
    "            similar = cbow_model.wv.most_similar(word, topn=3)\n",
    "            print(f\"\\nWords similar to '{word}':\")\n",
    "            for sim_word, score in similar:\n",
    "                print(f\"  {sim_word}: {score:.4f}\")\n",
    "        except:\n",
    "            print(f\"\\nNot enough data for '{word}' similarities\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== CBOW VS SKIP-GRAM COMPARISON ===\")\n",
    "\n",
    "# Train Skip-gram model for comparison\n",
    "print(\"Training Skip-gram model for comparison...\")\n",
    "skipgram_model = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=50,\n",
    "    window=3,\n",
    "    min_count=1,\n",
    "    workers=1,\n",
    "    sg=1,                # Skip-gram architecture\n",
    "    epochs=100\n",
    ")\n",
    "\n",
    "print(\"\\nComparing models on word 'fox':\")\n",
    "if 'fox' in cbow_model.wv and 'fox' in skipgram_model.wv:\n",
    "    try:\n",
    "        cbow_similar = cbow_model.wv.most_similar('fox', topn=2)\n",
    "        skipgram_similar = skipgram_model.wv.most_similar('fox', topn=2)\n",
    "        \n",
    "        print(\"CBOW similar words:\")\n",
    "        for word, score in cbow_similar:\n",
    "            print(f\"  {word}: {score:.4f}\")\n",
    "            \n",
    "        print(\"Skip-gram similar words:\")\n",
    "        for word, score in skipgram_similar:\n",
    "            print(f\"  {word}: {score:.4f}\")\n",
    "    except:\n",
    "        print(\"Not enough data for comparison\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== KEY CBOW INSIGHTS ===\")\n",
    "print(\"üéØ CBOW Architecture:\")\n",
    "print(\"‚Ä¢ Input: Context words ['the', 'quick', 'brown', 'jumps']\")\n",
    "print(\"‚Ä¢ Output: Target word 'fox'\")\n",
    "print(\"‚Ä¢ Learning: Predicts center word from surrounding context\")\n",
    "print()\n",
    "print(\"‚ö° CBOW Advantages:\")\n",
    "print(\"‚Ä¢ Faster training than Skip-gram\")\n",
    "print(\"‚Ä¢ Better for frequent words\")\n",
    "print(\"‚Ä¢ Good performance with smaller datasets\")\n",
    "print()\n",
    "print(\"üîç CBOW Process:\")\n",
    "print(\"1. Take context words around target\")\n",
    "print(\"2. Average their vectors\")\n",
    "print(\"3. Predict the target word\")\n",
    "print(\"4. Update weights based on prediction accuracy\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bb2901f5",
   "metadata": {},
   "source": [
    "sg: This is the crucial parameter for choosing between CBOW and Skip-gram:\n",
    "sg=0 (default): Trains a CBOW model.\n",
    "sg=1: Trains a Skip-gram model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097ac512",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "GloVe (Global Vectors for Word Representation)\n",
    "GloVe (Global Vectors for Word Representation) is an unsupervised learning model \n",
    "that generates word embeddings by combining the advantages of 2 major embedding\n",
    " approaches: \n",
    " global matrix factorization (like Latent Semantic Analysis) and\n",
    " local context window methods (like Word2Vec)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "2728bc9e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Latent Semantic Analysis (LSA): LSA is a technique that analyzes the co-occurrence\n",
    " of words in a corpus. It creates a large matrix where rows are words and columns\n",
    " are documents, and the values are word counts (or TF-IDF scores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b8fa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative GloVe implementation using available libraries\n",
    "# Since the glove package has build issues, we'll demonstrate GloVe concepts\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "print(\"=== GloVe Concept Demonstration ===\")\n",
    "print(\"Note: This demonstrates GloVe concepts using available libraries\")\n",
    "print()\n",
    "\n",
    "# Sample corpus for demonstration\n",
    "sentences = [\n",
    "    ['the', 'cat', 'sat', 'on', 'the', 'mat'],\n",
    "    ['a', 'dog', 'ran', 'down', 'the', 'street'],  \n",
    "    ['the', 'cat', 'was', 'chasing', 'a', 'mouse'],\n",
    "    ['the', 'dog', 'barked', 'at', 'the', 'cat'],\n",
    "    ['a', 'mouse', 'ran', 'from', 'the', 'cat']\n",
    "]\n",
    "\n",
    "# 1. Build co-occurrence matrix (core concept of GloVe)\n",
    "def build_cooccurrence_matrix(sentences, window_size=2):\n",
    "    \"\"\"Build word co-occurrence matrix like GloVe does\"\"\"\n",
    "    word_counts = Counter()\n",
    "    cooccurrence = defaultdict(Counter)\n",
    "    \n",
    "    # Count all words and build co-occurrence\n",
    "    for sentence in sentences:\n",
    "        for i, word in enumerate(sentence):\n",
    "            word_counts[word] += 1\n",
    "            \n",
    "            # Look at surrounding words within window\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(len(sentence), i + window_size + 1)\n",
    "            \n",
    "            for j in range(start, end):\n",
    "                if i != j:  # Don't count the word with itself\n",
    "                    context_word = sentence[j]\n",
    "                    distance = abs(i - j)\n",
    "                    weight = 1.0 / distance  # Closer words get higher weight\n",
    "                    cooccurrence[word][context_word] += weight\n",
    "    \n",
    "    return word_counts, cooccurrence\n",
    "\n",
    "# Build the co-occurrence matrix\n",
    "word_counts, cooccurrence = build_cooccurrence_matrix(sentences, window_size=2)\n",
    "\n",
    "print(\"Word frequencies:\")\n",
    "for word, count in sorted(word_counts.items()):\n",
    "    print(f\"  {word}: {count}\")\n",
    "\n",
    "print(f\"\\nCo-occurrence matrix built with {len(cooccurrence)} unique words\")\n",
    "\n",
    "# Show co-occurrence for 'cat'\n",
    "print(f\"\\nWords that co-occur with 'cat':\")\n",
    "if 'cat' in cooccurrence:\n",
    "    for word, weight in sorted(cooccurrence['cat'].items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  {word}: {weight:.2f}\")\n",
    "\n",
    "# 2. For comparison, let's also show Word2Vec results on the same data\n",
    "print(\"\\n=== Word2Vec Comparison ===\")\n",
    "model = Word2Vec(sentences, vector_size=50, window=2, min_count=1, workers=1)\n",
    "\n",
    "print(\"Words most similar to 'cat' (Word2Vec):\")\n",
    "try:\n",
    "    similar_words = model.wv.most_similar('cat', topn=3)\n",
    "    for word, similarity in similar_words:\n",
    "        print(f\"  {word}: {similarity:.3f}\")\n",
    "except:\n",
    "    print(\"  Not enough data for similarity calculation\")\n",
    "\n",
    "# 3. Show the concept of GloVe vs Word2Vec\n",
    "print(\"\\n=== Key Differences ===\")\n",
    "print(\"GloVe:\")\n",
    "print(\"  - Uses global co-occurrence statistics\")\n",
    "print(\"  - Builds explicit co-occurrence matrix\")  \n",
    "print(\"  - Combines global matrix factorization with local context\")\n",
    "\n",
    "print(\"\\nWord2Vec:\")\n",
    "print(\"  - Uses local context windows\")\n",
    "print(\"  - Predicts words from context (or vice versa)\")\n",
    "print(\"  - No explicit global co-occurrence matrix\")\n",
    "\n",
    "print(f\"\\nNote: To use actual GloVe embeddings, you can download pre-trained vectors\")\n",
    "print(\"from Stanford's GloVe website and load them with gensim.models.KeyedVectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35823b3c",
   "metadata": {},
   "source": [
    "4. ELMo\n",
    "ELMo was one of the first major breakthroughs in contextualized embeddings. It uses a bidirectional LSTM (Long Short-Term Memory) network to create a word‚Äôs vector representation. Unlike static embeddings, an ELMo vector for a word is a function of the entire sentence it appears in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227682fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELMo (Embeddings from Language Models) Example Program\n",
    "import numpy as np\n",
    "\n",
    "print(\"=== ELMo CONTEXTUALIZED EMBEDDINGS DEMONSTRATION ===\")\n",
    "print(\"ELMo: Context-dependent word representations\")\n",
    "print()\n",
    "\n",
    "# Check if ELMo libraries are available\n",
    "try:\n",
    "    # Try importing TensorFlow Hub (most common ELMo implementation)\n",
    "    import tensorflow_hub as hub\n",
    "    import tensorflow as tf\n",
    "    elmo_available = True\n",
    "    print(\"‚úÖ TensorFlow Hub available for ELMo\")\n",
    "except ImportError:\n",
    "    elmo_available = False\n",
    "    print(\"‚ùå TensorFlow Hub not available\")\n",
    "\n",
    "try:\n",
    "    # Alternative: allennlp ELMo\n",
    "    from allennlp.modules.elmo import Elmo, batch_to_ids\n",
    "    allennlp_available = True\n",
    "    print(\"‚úÖ AllenNLP ELMo available\")\n",
    "except ImportError:\n",
    "    allennlp_available = False\n",
    "    print(\"‚ùå AllenNLP not available\")\n",
    "\n",
    "print()\n",
    "\n",
    "if elmo_available:\n",
    "    print(\"=== TENSORFLOW HUB ELMo IMPLEMENTATION ===\")\n",
    "    try:\n",
    "        # Load pre-trained ELMo model from TensorFlow Hub\n",
    "        print(\"Loading ELMo model from TensorFlow Hub...\")\n",
    "        elmo_url = \"https://tfhub.dev/google/elmo/3\"\n",
    "        \n",
    "        # This is a simplified example - actual implementation would require download\n",
    "        print(\"Note: This requires internet connection and model download\")\n",
    "        print(\"Model URL:\", elmo_url)\n",
    "        \n",
    "        # Sample sentences showing context dependency\n",
    "        sentences = [\n",
    "            [\"The\", \"bank\", \"was\", \"built\", \"near\", \"the\", \"river\"],\n",
    "            [\"I\", \"need\", \"to\", \"go\", \"to\", \"the\", \"bank\", \"for\", \"money\"],\n",
    "            [\"The\", \"bat\", \"flew\", \"out\", \"of\", \"the\", \"cave\"],\n",
    "            [\"He\", \"hit\", \"the\", \"ball\", \"with\", \"a\", \"wooden\", \"bat\"]\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nExample sentences showing context dependency:\")\n",
    "        for i, sentence in enumerate(sentences, 1):\n",
    "            print(f\"{i}. {' '.join(sentence)}\")\n",
    "        \n",
    "        print(\"\\nELMo would generate different vectors for:\")\n",
    "        print(\"‚Ä¢ 'bank' in sentence 1 (river bank) vs sentence 2 (financial bank)\")\n",
    "        print(\"‚Ä¢ 'bat' in sentence 3 (animal) vs sentence 4 (sports equipment)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading TensorFlow Hub ELMo: {e}\")\n",
    "\n",
    "elif allennlp_available:\n",
    "    print(\"=== ALLENNLP ELMo IMPLEMENTATION ===\")\n",
    "    try:\n",
    "        # AllenNLP ELMo implementation\n",
    "        options_file = \"https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n",
    "        weight_file = \"https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n",
    "        \n",
    "        print(\"AllenNLP ELMo configuration:\")\n",
    "        print(f\"Options: {options_file}\")\n",
    "        print(f\"Weights: {weight_file}\")\n",
    "        print(\"Note: Actual usage requires downloading these files\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with AllenNLP ELMo: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"=== ELMo CONCEPT DEMONSTRATION ===\")\n",
    "    print(\"Since ELMo packages aren't available, let's demonstrate the concept\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== ELMo CONCEPT: CONTEXTUALIZED EMBEDDINGS ===\")\n",
    "\n",
    "# Demonstrate ELMo concept with examples\n",
    "print(\"üîç KEY CONCEPT: Same word, different meanings based on context\")\n",
    "print()\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"word\": \"bank\",\n",
    "        \"sentences\": [\n",
    "            \"The bank of the river was muddy\",\n",
    "            \"I deposited money at the bank\"\n",
    "        ],\n",
    "        \"meanings\": [\"geographical feature\", \"financial institution\"]\n",
    "    },\n",
    "    {\n",
    "        \"word\": \"bat\",\n",
    "        \"sentences\": [\n",
    "            \"The bat flew through the night\",\n",
    "            \"He swung the bat at the ball\"\n",
    "        ],\n",
    "        \"meanings\": [\"flying mammal\", \"sports equipment\"]\n",
    "    },\n",
    "    {\n",
    "        \"word\": \"rock\",\n",
    "        \"sentences\": [\n",
    "            \"The ship hit a large rock\",\n",
    "            \"Let's rock and roll tonight\"\n",
    "        ],\n",
    "        \"meanings\": [\"stone/mineral\", \"music/movement\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "for example in examples:\n",
    "    word = example[\"word\"]\n",
    "    sentences = example[\"sentences\"]\n",
    "    meanings = example[\"meanings\"]\n",
    "    \n",
    "    print(f\"Word: '{word}'\")\n",
    "    for i, (sentence, meaning) in enumerate(zip(sentences, meanings)):\n",
    "        print(f\"  {i+1}. \\\"{sentence}\\\"\")\n",
    "        print(f\"     ‚Üí Meaning: {meaning}\")\n",
    "        print(f\"     ‚Üí ELMo generates DIFFERENT vectors for '{word}' here\")\n",
    "    print()\n",
    "\n",
    "print(\"=== HOW ELMo WORKS ===\")\n",
    "print(\"üß† ARCHITECTURE:\")\n",
    "print(\"1. Bidirectional LSTM processes text in both directions\")\n",
    "print(\"2. Forward LSTM: reads left ‚Üí right\")\n",
    "print(\"3. Backward LSTM: reads right ‚Üê left\")\n",
    "print(\"4. Combines both directions for context-aware representation\")\n",
    "print()\n",
    "\n",
    "print(\"üìä ELMo PROCESS:\")\n",
    "sentence_example = \"The bank was built near the river\"\n",
    "words = sentence_example.split()\n",
    "\n",
    "print(f\"Example: \\\"{sentence_example}\\\"\")\n",
    "print()\n",
    "print(\"Forward LSTM (left to right):\")\n",
    "for i, word in enumerate(words):\n",
    "    context = \" \".join(words[:i+1])\n",
    "    print(f\"  Step {i+1}: {context} ‚Üí processes '{word}'\")\n",
    "\n",
    "print()\n",
    "print(\"Backward LSTM (right to left):\")\n",
    "for i, word in enumerate(reversed(words)):\n",
    "    remaining = words[len(words)-i-1:]\n",
    "    context = \" \".join(remaining)\n",
    "    print(f\"  Step {i+1}: {context} ‚Üê processes '{word}'\")\n",
    "\n",
    "print()\n",
    "print(\"Final representation for 'bank':\")\n",
    "print(\"  = combination of forward + backward representations\")\n",
    "print(\"  = captures that 'bank' is near 'river' (geographical context)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== ELMo VS STATIC EMBEDDINGS ===\")\n",
    "\n",
    "comparison_data = [\n",
    "    [\"Aspect\", \"Static Embeddings (Word2Vec/GloVe)\", \"ELMo\"],\n",
    "    [\"Context\", \"Same vector regardless of context\", \"Different vector per context\"],\n",
    "    [\"Polysemy\", \"One vector for all meanings\", \"Multiple vectors for different meanings\"],\n",
    "    [\"Example\", \"'bank' always same vector\", \"'bank' different in financial vs river context\"],\n",
    "    [\"Architecture\", \"Single embedding layer\", \"Bidirectional LSTM\"],\n",
    "    [\"Computation\", \"Fast lookup\", \"Requires forward pass through network\"],\n",
    "    [\"Memory\", \"Small (vocabulary √ó dimensions)\", \"Large (full neural network)\"]\n",
    "]\n",
    "\n",
    "for row in comparison_data:\n",
    "    if row[0] == \"Aspect\":  # Header\n",
    "        print(f\"{row[0]:<12} | {row[1]:<35} | {row[2]}\")\n",
    "        print(\"-\" * 85)\n",
    "    else:\n",
    "        print(f\"{row[0]:<12} | {row[1]:<35} | {row[2]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== PRACTICAL ELMo USAGE ===\")\n",
    "print(\"üíª INSTALLATION:\")\n",
    "print(\"pip install tensorflow-hub  # For TensorFlow Hub ELMo\")\n",
    "print(\"pip install allennlp        # For AllenNLP ELMo\")\n",
    "print()\n",
    "\n",
    "print(\"üîß BASIC USAGE TEMPLATE:\")\n",
    "print(\"\"\"\n",
    "# TensorFlow Hub approach\n",
    "import tensorflow_hub as hub\n",
    "elmo = hub.load(\"https://tfhub.dev/google/elmo/3\")\n",
    "embeddings = elmo([\"Hello world\", \"How are you\"])\n",
    "\n",
    "# AllenNLP approach  \n",
    "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
    "elmo = Elmo(options_file, weight_file, 2, dropout=0)\n",
    "character_ids = batch_to_ids(sentences)\n",
    "embeddings = elmo(character_ids)\n",
    "\"\"\")\n",
    "\n",
    "print(\"üéØ APPLICATIONS:\")\n",
    "print(\"‚Ä¢ Named Entity Recognition (NER)\")\n",
    "print(\"‚Ä¢ Part-of-Speech Tagging\")\n",
    "print(\"‚Ä¢ Sentiment Analysis\")\n",
    "print(\"‚Ä¢ Question Answering\")\n",
    "print(\"‚Ä¢ Any task requiring context understanding\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== ELMo ADVANTAGES & LIMITATIONS ===\")\n",
    "print(\"‚úÖ ADVANTAGES:\")\n",
    "print(\"‚Ä¢ Handles polysemy (multiple meanings)\")\n",
    "print(\"‚Ä¢ Context-dependent representations\")\n",
    "print(\"‚Ä¢ Improves downstream task performance\")\n",
    "print(\"‚Ä¢ Works with out-of-vocabulary words\")\n",
    "print()\n",
    "\n",
    "print(\"‚ö†Ô∏è LIMITATIONS:\")\n",
    "print(\"‚Ä¢ Computationally expensive\")\n",
    "print(\"‚Ä¢ Requires large memory\")\n",
    "print(\"‚Ä¢ Slower than static embeddings\")\n",
    "print(\"‚Ä¢ Superseded by Transformer models (BERT, GPT)\")\n",
    "\n",
    "print(f\"\\nüöÄ EVOLUTION:\")\n",
    "print(\"Word2Vec/GloVe ‚Üí ELMo ‚Üí BERT/GPT ‚Üí Modern Transformers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096a6e31",
   "metadata": {},
   "source": [
    "5. Transformer-Based Models: These models are the current state of the art for text representation. They rely on the Transformer architecture and its self-attention mechanism, which can weigh the importance of different words in a sentence when encoding a specific word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c970995f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT and Transformer-Based Models Example Program\n",
    "import numpy as np\n",
    "\n",
    "print(\"=== BERT & TRANSFORMER-BASED MODELS DEMONSTRATION ===\")\n",
    "print(\"BERT: Bidirectional Encoder Representations from Transformers\")\n",
    "print()\n",
    "\n",
    "# Check if transformers library is available\n",
    "try:\n",
    "    from transformers import BertTokenizer, BertModel, AutoTokenizer, AutoModel\n",
    "    import torch\n",
    "    transformers_available = True\n",
    "    print(\"‚úÖ Transformers library available\")\n",
    "except ImportError:\n",
    "    transformers_available = False\n",
    "    print(\"‚ùå Transformers library not available\")\n",
    "    print(\"   Install with: pip install transformers torch\")\n",
    "\n",
    "print()\n",
    "\n",
    "if transformers_available:\n",
    "    print(\"=== BERT MODEL IMPLEMENTATION ===\")\n",
    "    try:\n",
    "        # Load pre-trained BERT model and tokenizer\n",
    "        print(\"Loading BERT-base-uncased model...\")\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        print(\"‚úÖ BERT model loaded successfully!\")\n",
    "        print(f\"Model: {model.config.name_or_path}\")\n",
    "        print(f\"Hidden size: {model.config.hidden_size}\")\n",
    "        print(f\"Number of layers: {model.config.num_hidden_layers}\")\n",
    "        print(f\"Number of attention heads: {model.config.num_attention_heads}\")\n",
    "        \n",
    "        # Example sentences showing context dependency\n",
    "        sentences = [\n",
    "            \"The bank near the river is beautiful\",\n",
    "            \"I need to visit the bank for a loan\",\n",
    "            \"The bat flew out of the cave\",\n",
    "            \"He hit a home run with the bat\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\n=== BERT TOKENIZATION ===\")\n",
    "        for i, sentence in enumerate(sentences[:2], 1):\n",
    "            print(f\"Sentence {i}: \\\"{sentence}\\\"\")\n",
    "            \n",
    "            # Tokenize\n",
    "            tokens = tokenizer.tokenize(sentence)\n",
    "            print(f\"  Tokens: {tokens}\")\n",
    "            \n",
    "            # Convert to input IDs\n",
    "            input_ids = tokenizer.encode(sentence, add_special_tokens=True)\n",
    "            print(f\"  Input IDs: {input_ids}\")\n",
    "            \n",
    "            # Decode back to text\n",
    "            decoded = tokenizer.decode(input_ids)\n",
    "            print(f\"  Decoded: \\\"{decoded}\\\"\")\n",
    "            print()\n",
    "        \n",
    "        print(\"=== BERT EMBEDDINGS ===\")\n",
    "        # Get embeddings for a sentence\n",
    "        sentence = \"The bank near the river is beautiful\"\n",
    "        print(f\"Analyzing: \\\"{sentence}\\\"\")\n",
    "        \n",
    "        # Tokenize and prepare inputs\n",
    "        inputs = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)\n",
    "        \n",
    "        # Get BERT embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            embeddings = outputs.last_hidden_state\n",
    "        \n",
    "        print(f\"Embedding shape: {embeddings.shape}\")\n",
    "        print(f\"Sequence length: {embeddings.shape[1]} tokens\")\n",
    "        print(f\"Hidden dimension: {embeddings.shape[2]}\")\n",
    "        \n",
    "        # Show embeddings for the word \"bank\"\n",
    "        tokens = tokenizer.tokenize(sentence)\n",
    "        if 'bank' in tokens:\n",
    "            bank_idx = tokens.index('bank') + 1  # +1 for [CLS] token\n",
    "            bank_embedding = embeddings[0, bank_idx, :]\n",
    "            print(f\"\\nEmbedding for 'bank' (first 10 dimensions):\")\n",
    "            print(f\"  {bank_embedding[:10].numpy()}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with BERT implementation: {e}\")\n",
    "        print(\"Note: BERT models are large and require significant memory\")\n",
    "\n",
    "else:\n",
    "    print(\"=== TRANSFORMER CONCEPT DEMONSTRATION ===\")\n",
    "    print(\"Since transformers library isn't available, let's demonstrate concepts\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== TRANSFORMER ARCHITECTURE EXPLAINED ===\")\n",
    "\n",
    "print(\"üèóÔ∏è KEY COMPONENTS:\")\n",
    "print(\"1. Self-Attention Mechanism\")\n",
    "print(\"2. Multi-Head Attention\")\n",
    "print(\"3. Position Encodings\")\n",
    "print(\"4. Feed-Forward Networks\")\n",
    "print(\"5. Layer Normalization\")\n",
    "print(\"6. Residual Connections\")\n",
    "print()\n",
    "\n",
    "print(\"üîç SELF-ATTENTION CONCEPT:\")\n",
    "sentence = \"The cat sat on the mat\"\n",
    "words = sentence.split()\n",
    "\n",
    "print(f\"Example: \\\"{sentence}\\\"\")\n",
    "print(\"Self-attention asks: How much should each word attend to every other word?\")\n",
    "print()\n",
    "\n",
    "# Simulate attention weights (simplified)\n",
    "print(\"Attention matrix (simplified concept):\")\n",
    "print(\"     \", \" \".join(f\"{word:>6}\" for word in words))\n",
    "\n",
    "# Mock attention weights for demonstration\n",
    "attention_weights = [\n",
    "    [0.8, 0.1, 0.05, 0.02, 0.02, 0.01],  # The\n",
    "    [0.2, 0.6, 0.1, 0.05, 0.03, 0.02],   # cat\n",
    "    [0.1, 0.3, 0.4, 0.1, 0.05, 0.05],    # sat\n",
    "    [0.05, 0.1, 0.2, 0.5, 0.1, 0.05],    # on\n",
    "    [0.1, 0.05, 0.05, 0.1, 0.6, 0.1],    # the\n",
    "    [0.05, 0.2, 0.1, 0.1, 0.1, 0.45]     # mat\n",
    "]\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    weights_str = \" \".join(f\"{w:>6.2f}\" for w in attention_weights[i])\n",
    "    print(f\"{word:>4}: {weights_str}\")\n",
    "\n",
    "print()\n",
    "print(\"High values = strong attention (e.g., 'cat' attends strongly to 'sat')\")\n",
    "print(\"Low values = weak attention\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== BERT SPECIAL FEATURES ===\")\n",
    "\n",
    "print(\"üéØ BIDIRECTIONAL CONTEXT:\")\n",
    "print(\"Unlike ELMo, BERT sees the ENTIRE sentence at once\")\n",
    "print(\"Example: 'The [MASK] sat on the mat'\")\n",
    "print(\"‚Ä¢ BERT can use both 'The' and 'sat on the mat' to predict [MASK]\")\n",
    "print(\"‚Ä¢ This bidirectional understanding is BERT's key innovation\")\n",
    "print()\n",
    "\n",
    "print(\"üî§ SPECIAL TOKENS:\")\n",
    "special_tokens = [\n",
    "    (\"[CLS]\", \"Classification token - represents entire sentence\"),\n",
    "    (\"[SEP]\", \"Separator token - separates sentences\"),\n",
    "    (\"[MASK]\", \"Mask token - used for masked language modeling\"),\n",
    "    (\"[PAD]\", \"Padding token - used to make sequences same length\"),\n",
    "    (\"[UNK]\", \"Unknown token - for words not in vocabulary\")\n",
    "]\n",
    "\n",
    "for token, description in special_tokens:\n",
    "    print(f\"  {token:>6}: {description}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== TRANSFORMER VS PREVIOUS MODELS ===\")\n",
    "\n",
    "comparison = [\n",
    "    [\"Feature\", \"Word2Vec/GloVe\", \"ELMo\", \"BERT/Transformers\"],\n",
    "    [\"Context\", \"None\", \"Sequential\", \"Bidirectional\"],\n",
    "    [\"Architecture\", \"Shallow\", \"LSTM\", \"Self-Attention\"],\n",
    "    [\"Training\", \"Word prediction\", \"Language modeling\", \"Masked LM + NSP\"],\n",
    "    [\"Parallelization\", \"Not applicable\", \"Sequential\", \"Fully parallel\"],\n",
    "    [\"Context window\", \"Fixed\", \"Sequential\", \"Full sequence\"],\n",
    "    [\"Performance\", \"Good\", \"Better\", \"State-of-the-art\"]\n",
    "]\n",
    "\n",
    "for row in comparison:\n",
    "    if row[0] == \"Feature\":  # Header\n",
    "        print(f\"{row[0]:<15} | {row[1]:<15} | {row[2]:<12} | {row[3]}\")\n",
    "        print(\"-\" * 70)\n",
    "    else:\n",
    "        print(f\"{row[0]:<15} | {row[1]:<15} | {row[2]:<12} | {row[3]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== BERT PRE-TRAINING TASKS ===\")\n",
    "\n",
    "print(\"üé≠ MASKED LANGUAGE MODELING (MLM):\")\n",
    "examples = [\n",
    "    \"Original: The cat sat on the mat\",\n",
    "    \"Masked:   The [MASK] sat on the mat\",\n",
    "    \"Task:     Predict 'cat' using bidirectional context\"\n",
    "]\n",
    "for example in examples:\n",
    "    print(f\"  {example}\")\n",
    "\n",
    "print()\n",
    "print(\"üîó NEXT SENTENCE PREDICTION (NSP):\")\n",
    "examples = [\n",
    "    \"Sentence A: The cat sat on the mat\",\n",
    "    \"Sentence B: It was very comfortable\",\n",
    "    \"Task:       Predict if B follows A (IsNext vs NotNext)\"\n",
    "]\n",
    "for example in examples:\n",
    "    print(f\"  {example}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== POPULAR TRANSFORMER MODELS ===\")\n",
    "\n",
    "models = [\n",
    "    (\"BERT\", \"Bidirectional encoder, great for understanding\"),\n",
    "    (\"GPT\", \"Autoregressive decoder, great for generation\"),\n",
    "    (\"T5\", \"Text-to-text transformer, versatile\"),\n",
    "    (\"RoBERTa\", \"Robustly optimized BERT, improved training\"),\n",
    "    (\"ELECTRA\", \"Efficient pre-training, faster than BERT\"),\n",
    "    (\"DistilBERT\", \"Distilled BERT, smaller and faster\"),\n",
    "    (\"ALBERT\", \"A Lite BERT, parameter sharing\"),\n",
    "    (\"DeBERTa\", \"Decoding-enhanced BERT with attention\")\n",
    "]\n",
    "\n",
    "for model, description in models:\n",
    "    print(f\"  {model:<12}: {description}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== PRACTICAL TRANSFORMER USAGE ===\")\n",
    "\n",
    "print(\"üíª INSTALLATION:\")\n",
    "print(\"pip install transformers torch\")\n",
    "print(\"pip install transformers[torch]  # With PyTorch\")\n",
    "print()\n",
    "\n",
    "print(\"üîß BASIC USAGE TEMPLATE:\")\n",
    "print(\"\"\"\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load any transformer model\n",
    "model_name = 'bert-base-uncased'  # or 'gpt2', 'roberta-base', etc.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize and encode\n",
    "text = \"Hello, world!\"\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "# Get embeddings\n",
    "outputs = model(**inputs)\n",
    "embeddings = outputs.last_hidden_state\n",
    "\"\"\")\n",
    "\n",
    "print(\"üéØ APPLICATIONS:\")\n",
    "applications = [\n",
    "    \"Text Classification (sentiment, spam detection)\",\n",
    "    \"Named Entity Recognition (NER)\",\n",
    "    \"Question Answering\",\n",
    "    \"Text Summarization\",\n",
    "    \"Machine Translation\",\n",
    "    \"Text Generation\",\n",
    "    \"Semantic Search\",\n",
    "    \"Language Understanding\"\n",
    "]\n",
    "\n",
    "for app in applications:\n",
    "    print(f\"  ‚Ä¢ {app}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== TRANSFORMER ADVANTAGES & LIMITATIONS ===\")\n",
    "\n",
    "print(\"‚úÖ ADVANTAGES:\")\n",
    "advantages = [\n",
    "    \"State-of-the-art performance on most NLP tasks\",\n",
    "    \"Bidirectional context understanding\",\n",
    "    \"Transfer learning capabilities\",\n",
    "    \"Parallel processing during training\",\n",
    "    \"Rich pre-trained models available\",\n",
    "    \"Fine-tuning for specific tasks\"\n",
    "]\n",
    "\n",
    "for adv in advantages:\n",
    "    print(f\"  ‚Ä¢ {adv}\")\n",
    "\n",
    "print()\n",
    "print(\"‚ö†Ô∏è LIMITATIONS:\")\n",
    "limitations = [\n",
    "    \"Very large model sizes (millions/billions of parameters)\",\n",
    "    \"High computational requirements\",\n",
    "    \"Significant memory usage\",\n",
    "    \"Long training times\",\n",
    "    \"Quadratic complexity with sequence length\",\n",
    "    \"Black box nature (interpretability challenges)\"\n",
    "]\n",
    "\n",
    "for lim in limitations:\n",
    "    print(f\"  ‚Ä¢ {lim}\")\n",
    "\n",
    "print(f\"\\nüöÄ EVOLUTION TIMELINE:\")\n",
    "timeline = [\n",
    "    \"2017: Transformer architecture introduced\",\n",
    "    \"2018: BERT revolutionizes NLP\",\n",
    "    \"2019: GPT-2, RoBERTa, ALBERT\",\n",
    "    \"2020: GPT-3, T5, DeBERTa\",\n",
    "    \"2021: GPT-3.5, PaLM\",\n",
    "    \"2022: ChatGPT, GPT-4\",\n",
    "    \"2023+: Even larger and more efficient models\"\n",
    "]\n",
    "\n",
    "for year in timeline:\n",
    "    print(f\"  {year}\")\n",
    "\n",
    "print(f\"\\nüí° KEY INSIGHT:\")\n",
    "print(\"Transformers revolutionized NLP by enabling bidirectional context\")\n",
    "print(\"understanding and massive scale pre-training with transfer learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fb7be1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
